<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Diffusion Transformers with Representation Autoencoders</title>
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>Diffusion Transformers asd with Representation Autoencoders</h1>
              <p>Representation Autoencoders (RAEs) reuse frozen vision foundation encoders together with lightweight decoders to provide high-fidelity, semantically rich latents for diffusion transformers.</p>
              <div class="button-container">
                <a href="ReprDiT_Boyang (24).pdf" class="button">Paper</a>
                <a href="https://github.com/rae-dit/RAE" class="button">Code</a>
                <a href="#" class="button">ðŸ¤— Models</a>
              </div>
            </div>
            <div class="header-image">
                <img src="images/rae/RAE_teaser.png" alt="Representation Autoencoder teaser" class="teaser-image">
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="http://bytetriper.github.io/academica/" class="author-link">Boyang Zheng</a></p>
                    <p><a href="https://willisma.github.io/" class="author-link">Nanye Ma</a></p>
                    <p><a href="https://tsb0601.github.io/" class="author-link">Shengbang Tong</a></p>
                    <p><a href="https://www.sainingxie.com/" class="author-link">Saining Xie</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p><a href="https://cs.nyu.edu/home/index.html" class="affiliation-link">New York University</a></p>
                </div>
                <div class="byline-column">
                    <h3>Resources</h3>
                    <p><a href="ReprDiT_Boyang (24).pdf" class="affiliation-link">Paper (PDF)</a></p>
                    <p><a href="https://github.com/rae-dit/RAE" class="affiliation-link">Code Repository</a></p>
                    <p><a href="#" class="affiliation-link">Hugging Face Models</a></p>
                </div>
            </div>
        </div>
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#overview">Overview</a></div>
                <div><a href="#autoencoders">Representation Autoencoders</a></div>
                <div><a href="#diffusion">Diffusion Transformers</a></div>
                <div><a href="#scaling">Scaling and Efficiency</a></div>
                <div><a href="#data">Data and Alignment</a></div>
                <div><a href="#conclusion">Conclusion</a></div>
            </nav>
        </d-contents>
        <section id="overview">
            <h2>Overview</h2>
            <p>
                We present <strong>R</strong>epresentation <strong>A</strong>uto<strong>E</strong>ncoders (RAEs), a new class of autoencoders that leverage frozen, pretrained representation encoders to produce high-fidelity, semantically rich latent spaces for generative modeling. Frozen representation encoders such as DINOv2<d-cite key="peebles2023scalable"></d-cite> and SigLIP produce feature tokens that retain sharp semantics even without task-specific fine-tuning. Representation Autoencoders (RAEs) keep these encoders fixed and learn a lightweight ViT decoder that faithfully reconstructs images, providing an information-dense latent space.</p>
            <p>Training diffusion transformers directly on RAE latents delivers fast and high-quality generation: we reach FID 1.53 on ImageNet 256Ã—256 and FID 1.13 on 512Ã—512 with only 80 training epochsâ€”an order of magnitude fewer updates than prior autoencoder pipelines.</p>
            <ul>
                <li>High-fidelity reconstructions from frozen encoders enable semantic-rich latents without expensive posterior collapse fixes.</li>
                <li>Diffusion transformers converge 10Ã— faster while matching or exceeding state-of-the-art quality.</li>
                <li>Careful scaling, data balancing, and multimodal alignment make the system robust for downstream generation.</li>
            </ul>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/intro.png" alt="Pipeline overview for representation autoencoders">
                    <figcaption>Pipeline overview: a frozen vision encoder writes semantic tokens, a lightweight decoder reconstructs pixels, and diffusion transformers operate in the latent space.</figcaption>
                </figure>
            </d-figure>
        </section>
        <section id="autoencoders">
            <h2>Representation Autoencoders</h2> 
            <p>RAEs freeze pretrained vision encoders and optimize only a compact decoder to reconstruct the input image. Despite never being trained for pixel fidelity, frozen DINOv2 and SigLIP encoders achieve striking reconstruction quality, preserving both fine detail and global semantics.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/reprdit_recon.png" alt="Reconstruction examples produced by RAEs">
                    <figcaption>Reconstruction examples with a frozen DINOv2-B encoder. Even small RAEs rival SD-VAE quality while keeping rich semantic tokens.</figcaption>
                </figure>
            </d-figure>
            <p>Quantitatively, RAEs deliver strong perceptual scores and ImageNet linear probe accuracy without the heavy compression used in variational autoencoders. They also cut reconstruction compute by more than an order of magnitude compared to SD-VAE latents.</p>
            <div>
                <table class="display-table" style="margin-top: 25px;">
                    <thead>
                        <tr>
                            <th>Encoder</th>
                            <th>Decoder</th>
                            <th>rFID â†“</th>
                            <th>Top-1 Acc. â†‘</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="text-align: left;">DINOv2-B/14</td>
                            <td style="text-align: left;">ViT-B</td>
                            <td>0.49</td>
                            <td>84.5%</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">SigLIP-B/16</td>
                            <td style="text-align: left;">ViT-B</td>
                            <td>0.38</td>
                            <td>82.1%</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">MAE-B/16</td>
                            <td style="text-align: left;">ViT-B</td>
                            <td>0.16</td>
                            <td>67.8%</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">SD-VAE</td>
                            <td style="text-align: left;">â€”</td>
                            <td>0.62</td>
                            <td>~8%</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div>
                <table class="display-table" style="margin-top: 25px;">
                    <thead>
                        <tr>
                            <th>Decoder</th>
                            <th>rFID â†“</th>
                            <th>GFLOPs â†“</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="text-align: left;">ViT-B</td>
                            <td>0.58</td>
                            <td>22.2</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">ViT-L</td>
                            <td>0.50</td>
                            <td>78.1</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">ViT-XL</td>
                            <td>0.49</td>
                            <td>106.7</td>
                        </tr>
                        <tr>
                            <td style="text-align: left;">SD-VAE</td>
                            <td>0.62</td>
                            <td>310.4</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>
        <section id="diffusion">
            <h2>Diffusion Transformers on RAE Latents</h2>
            <p>Building on the expressive latent space, we adapt diffusion transformers (DiTs) to operate on RAE tokens. Token-aware noise schedules and objective reweighting close the gap between semantic latents and classical pixel-space training.</p>
            <p>The resulting models converge dramatically faster than diffusion on SD-VAE latents and reach new best FID scores with the same computational budget.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/convergence_compare.png" alt="Convergence comparison between RAEs and SD-VAE">
                    <figcaption>Training convergence on ImageNet 256Ã—256: RAEs reach strong sample quality in one tenth the number of updates.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/arch_teaser.png" alt="DiT architecture operating on RAE tokens">
                    <figcaption>DiT architecture tailored to RAE tokens with resolution-aware schedule shifting and decoder-aware sampling.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/comparison.PNG" alt="Sample comparison across generative models">
                    <figcaption>Sample quality comparison against state-of-the-art diffusion baselines on ImageNet 256Ã—256.</figcaption>
                </figure>
            </d-figure>
        </section>
        <section id="scaling">
            <h2>Scaling and Efficiency</h2>
            <p>Because the encoder is frozen, scaling RAEs primarily increases decoder width and depth, keeping training efficient. We explore decoder scaling and demonstrate smooth improvements without overfitting.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/model_width.png" alt="Decoder width scaling trends">
                    <figcaption>Decoder width scaling: wider RAEs steadily improve reconstruction fidelity while remaining compute-friendly.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/overfit_width.png" alt="Overfitting analysis for different decoder widths">
                    <figcaption>Overfitting analysis: RAEs maintain reconstruction quality across decoder sizes without collapsing semantics.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/recon_grid.png" alt="Reconstruction grid across model scales">
                    <figcaption>Qualitative reconstructions across model scales show consistent detail preservation.</figcaption>
                </figure>
            </d-figure>
        </section>
        <section id="data">
            <h2>Data and Alignment</h2>
            <p>Balanced data curation and alignment are key to stable training. We mix curated subsets to cover underrepresented concepts, and we track dataset composition and usage during large-scale sampling.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/data_mixture_ratio_w_avg_score.png" alt="Balanced data mixture for training">
                    <figcaption>Balanced mixture of curated datasets keeps ImageNet-style training stable across tens of billions of tokens.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/Cumulative_Sum_of_Counts.png" alt="Cumulative data usage statistics">
                    <figcaption>Cumulative data coverage shows how our sampler maintains diversity during long training runs.</figcaption>
                </figure>
            </d-figure>
            <p>We also support multimodal prompting for downstream creative use cases.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/sysprompt.jpg" alt="Multimodal interface for RAE diffusion models">
                    <figcaption>System prompt interface for guiding diffusion outputs with multimodal instructions.</figcaption>
                </figure>
            </d-figure>
        </section>
        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Representation Autoencoders convert strong vision foundation models into high-fidelity latent spaces that unlock fast, accurate diffusion transformers. Efficient scaling, careful data balancing, and practical interfaces make the approach a compelling alternative to traditional VAE pipelines.</p>
            <p>Explore the paper, code, and model checkpoints to build on RAEs for your own generative modeling projects.</p>
        </section>
    </d-article>
    </body>
</html>
