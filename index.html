<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Diffusion Transformers with Representation Autoencoders</title>
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[','\\]'], ['$$','$$']],
      packages: {'[+]': ['ams', 'textmacros']}  // enables \text
    },
    svg: { fontCache: 'global' }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>Diffusion Transformers with Representation Autoencoders</h1>
              <p>Representation Autoencoders (RAEs) reuse frozen vision foundation encoders together with lightweight decoders to provide high-fidelity, semantically rich latents for diffusion transformers.</p>
              <div class="button-container">
                <a href="ReprDiT_Boyang (24).pdf" class="button">Paper</a>
                <a href="https://github.com/rae-dit/RAE" class="button">Code</a>
                <a href="#" class="button">ü§ó Models</a>
              </div>
            </div>
            <!-- <div class="header-image">
                <img src="images/rae/RAE_teaser1.png" alt="Representation Autoencoder teaser" class="teaser-image">
            </div> -->
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="http://bytetriper.github.io/academica/" class="author-link">Boyang Zheng</a></p>
                    <p><a href="https://willisma.github.io/" class="author-link">Nanye Ma</a></p>
                    <p><a href="https://tsb0601.github.io/" class="author-link">Shengbang Tong</a></p>
                    <p><a href="https://www.sainingxie.com/" class="author-link">Saining Xie</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p><a href="https://cs.nyu.edu/home/index.html" class="affiliation-link">New York University</a></p>
                </div>
                <div class="byline-column">
                    <h3>Resources</h3>
                    <p><a href="ReprDiT_Boyang (24).pdf" class="affiliation-link">Paper (PDF)</a></p>
                    <p><a href="https://github.com/rae-dit/RAE" class="affiliation-link">Code Repository</a></p>
                    <p><a href="#" class="affiliation-link">Hugging Face Models</a></p>
                </div>
            </div>
        </div>
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#overview">Overview</a></div>
                <div><a href="#autoencoders">Representation Autoencoders</a></div>
                <div><a href="#diffusion">Diffusion Transformers</a></div>
                <div><a href="#scaling">Scaling and Efficiency</a></div>
                <div><a href="#data">Data and Alignment</a></div>
                <div><a href="#conclusion">Conclusion</a></div>
            </nav>
        </d-contents>
        <section id="overview">
            <h2>Overview</h2>
            <p>

                Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT)<d-cite key="peebles2023scalable"></d-cite>;
                however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder<d-cite key="VAE"></d-cite>, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality.
                In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO<d-cite key="DINO"></d-cite>, SigLIP<d-cite key="siglip2"></d-cite>, MAE<d-cite key="MAE"></d-cite>) paired with trained decoders, forming what we term <strong>R</strong>epresentation <strong>A</strong>uto<strong>e</strong>ncoders (RAEs).
            </p>
                These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture.
                Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically.
            </p>
            <p>
                Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: <strong>1.51</strong> FID at 256x256 (no guidance) and <strong>1.16 / 1.13</strong> at 256x256 and 512x512 (with guidance). 
                RAE offers clear advantages and should be the new default for diffusion transformer training. 
            </p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/intro.png" alt="Pipeline overview for representation autoencoders">
                    <figcaption>Pipeline overview: a frozen vision encoder writes semantic tokens, a lightweight decoder reconstructs pixels, and diffusion transformers operate in the latent space.</figcaption>
                </figure>
            </d-figure>
            <p>
                As with training VAEs, there are two questions to answer:
                <ul>
                    <li><b>How well can RAE reconstruct the input image?</b></li>
                    <li><b>How well can diffusion transformers operate within the RAE latent space?</b></li>
                </ul>
            </p>
        </section>
        <section id="autoencoders">
            <h2>High Fidelity Reconstruction from RAE</h2> 
            <p>   We challenge the common assumption that pretrained representation encoders, such as DINOv2 and SigLIP2, are unsuitable for the reconstruction task because they <em>‚Äúemphasize high-level semantics while downplaying low-level details‚Äù</em> [1, 2]. We show that, with a properly trained decoder, frozen SEM can in fact serve as strong encoders for the diffusion latent space. Our <strong>Representation Autoencoders (RAE)</strong> pair frozen, pretrained SEM with a ViT decoder, yielding reconstructions on par with‚Äîor even better than‚ÄîSD-VAE. More importantly, RAEs alleviate the fundamental limitations of VAEs [3], whose heavily compressed latent space (e.g., SD-VAE maps 256<sup>2</sup> images to 32<sup>2</sup>√ó4 [4, 5] latents) restricts reconstruction fidelity and, more importantly, representation quality.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/reprdit_recon.png" alt="Reconstruction examples produced by RAEs">
                    <figcaption>Reconstruction examples with a frozen DINOv2-B encoder. Even small RAEs rival SD-VAE quality while keeping rich semantic tokens.</figcaption>
                </figure>
            </d-figure>
            <h3>Reconstruction, scaling, and representation.</h3>
            <p>
            As shown below, RAEs achieve consistently better reconstruction quality (rFID) than SD-VAE.
            For instance, RAE with MAE-B/16 reaches an rFID of 0.16, clearly outperforming SD-VAE and challenging the assumption that representation encoders cannot recover pixel-level detail.
            </p>

            <p>
            We next study the scaling behavior of both encoders and decoders. As shown in Table&nbsp;1c, reconstruction quality remains stable across DINOv2-S, B, and L, indicating that even small representation encoder models preserve sufficient low-level detail for decoding.
            On the decoder side (Table&nbsp;1b), increasing capacity consistently improves rFID: from 0.58 with ViT-B to 0.49 with ViT-XL.
            Importantly, ViT-B already outperforms SD-VAE while being 14√ó more efficient in GFLOPs, and ViT-XL further improves quality at only one-third of SD-VAE‚Äôs cost.
            </p>
<table style="width:100%; margin-top:25px; border-collapse:separate; border-spacing:12px;">
  <tr>
    <!-- (a) Larger decoders -->
    <td style="vertical-align:top; width:33%;">
      <figure>
        <table class="display-table" style="width:100%;">
          <tr>
            <th>Decoder</th>
            <th>rFID</th>
            <th>GFLOPs</th>
          </tr>
          <tr>
            <td style="text-align:left;">ViT-B</td>
            <td>0.58</td>
            <td>22.2</td>
          </tr>
          <tr>
            <td style="text-align:left;">ViT-L</td>
            <td>0.50</td>
            <td>78.1</td>
          </tr>
          <tr>
            <td style="text-align:left;">ViT-XL</td>
            <td><strong>0.49</strong></td>
            <td>106.7</td>
          </tr>
          <tr style="background-color:#f0f0f0; color:#666;">
            <td style="text-align:left;">SD-VAE</td>
            <td>0.62</td>
            <td>310.4</td>
          </tr>
        </table>
        <figcaption style="text-align:center; margin-bottom:6px;">
          Larger decoders improve rFID while remaining much more efficient than VAEs.
        </figcaption>
      </figure>
    </td>

    <!-- (b) Encoder scaling -->
    <td style="vertical-align:top; width:33%;">
      <figure>
        <table class="display-table" style="width:100%;">
          <tr>
            <th>Encoder</th>
            <th>rFID</th>
          </tr>
          <tr>
            <td style="text-align:left;">DINOv2-S</td>
            <td>0.52</td>
          </tr>
          <tr>
            <td style="text-align:left;">DINOv2-B</td>
            <td><strong>0.49</strong></td>
          </tr>
          <tr>
            <td style="text-align:left;">DINOv2-L</td>
            <td>0.52</td>
          </tr>
        </table>
            <figcaption style="text-align:center; margin-bottom:6px;">
          Encoder scaling ‚Äî rFID remains stable across RAE sizes.
        </figcaption>
      </figure>
    </td>

    <!-- (c) Representation quality -->
    <td style="vertical-align:top; width:33%;">
      <figure>
        <table class="display-table" style="width:100%;">
          <tr>
            <th>Model</th>
            <th>Top-1&nbsp;Acc.</th>
          </tr>
          <tr>
            <td style="text-align:left;">DINOv2-B</td>
            <td><strong>84.5</strong></td>
          </tr>
          <tr>
            <td style="text-align:left;">SigLIP-B</td>
            <td>79.1</td>
          </tr>
          <tr>
            <td style="text-align:left;">MAE-B</td>
            <td>68.0</td>
          </tr>
          <tr style="background-color:#f0f0f0; color:#666;">
            <td style="text-align:left;">SD-VAE</td>
            <td>8.0</td>
          </tr>
        </table>
        <figcaption style="text-align:center; margin-bottom:6px;">
          RAEs have much higher linear probing accuracy than VAEs.
        </figcaption>
      </figure>
    </td>
  </tr>
</table>

<table style="width:100%; margin-top:25px; margin-left: -20px; border-collapse:separate; border-spacing:12px;">
  <tr>
    <!-- Left column: text -->
    <td style="vertical-align:top; width:60%; text-align:justify; font-size: 1em;">
      <p>
        We also evaluate representation quality via linear probing on ImageNet-1K in Table&nbsp;1d.
        Because RAEs use frozen pretrained encoders, they directly inherit the representations of the underlying encoders.
        Since RAEs use frozen pretrained encoders, they retain the strong representations of the underlying representation encoders.
        In contrast, SD-VAE achieves only approximately 8% accuracy.
      </p>
    </td>

    <!-- Right column: table with caption -->
    <td style="vertical-align:top; width:40%;">
      <figure>
        <table class="display-table" style="width:100%;">
          <tr>
            <th>Model</th>
            <th>Top-1&nbsp;Acc.</th>
          </tr>
          <tr>
            <td style="text-align:left;">DINOv2-B</td>
            <td><strong>84.5</strong></td>
          </tr>
          <tr>
            <td style="text-align:left;">SigLIP-B</td>
            <td>79.1</td>
          </tr>
          <tr>
            <td style="text-align:left;">MAE-B</td>
            <td>68.0</td>
          </tr>
          <tr style="background-color:#f0f0f0; color:#666;">
            <td style="text-align:left;">SD-VAE</td>
            <td>8.0</td>
          </tr>
        </table>
                <figcaption style="text-align:center; margin-bottom:6px;">
          RAEs have much higher linear probing accuracy than VAEs.
        </figcaption>
      </figure>
    </td>
  </tr>
</table>


        


        </section>
        <section id="diffusion">
            <h2>Taming Diffusion Transformers for RAE</h2>
            <p>
              With RAE demonstrating good reconstruction quality, we now proceed to investigate the <em>diffusability</em><dite key="ImprovDiffus"></dite> of its latent space;
              that is, how easily its latent distribution can be modeled by a diffusion model, and how good the generation performance can be.
              Empirically, despite the superior reconstruction quality of MAE encoders,
              we found that DINOv2 produces the strongest generation results, so we adopt it as the default, unless otherwise specified.
            </p>
            <p>
              Following standard practice, we adopt the flow matching objective<dite key="fm"></dite><dite key="rf"></dite> to train the diffusion model, for which we use LightningDiT<dite key="lgt"></dite>, a variant of DiT, as our model backbone.
            </p>
            <p>
              We adopt a patch size of 1, which results in a token length of 256 for all RAEs on 256x256 images, matching the sequence length used by VAE-based DiTs. We note that, Since the
              computational cost of DiT depends primarily on the sequence length, <b>using RAE latents on DiTs will not incur additional computational cost compared to using VAE latents.</b>
            </p>
            <div style="max-width: 1000px; margin: 0 auto;">
              <table style="width:100%; margin-top:25px; margin-left: -20px; border-collapse:separate; border-spacing:12px; border: none;">
                <tr>
                  <!-- Left column: text -->
                  <td style="vertical-align:top; width:60%; text-align:justify; font-size: 1em;">
                    <p>
                      <b>DiT Does Not Work Out of the Box.</b> Surprisingly, the standard diffusion recipe fails with RAE: 
                      training directly on RAE latents causes a small backbone such as DiT-S to completely fail, while a larger backbone like DiT-XL significantly underperforms it's counterpart with the SD-VAE latents.
                      To investigate this observation, we raise several hypotheses detailed below, which we will discuss in the following sections:
                    </p>
                  </td>
                  <!-- Right column: table -->
                  <td style="vertical-align:top; width:40%;">
                    <figure>
                      <table class="display-table" style="width:100%;">
                        <tr>
                          <th>&nbsp;</th>
                          <th style="text-align:center;">RAE</th>
                          <th style="text-align:center;">SD-VAE</th>
                        </tr>
                        <tr>
                          <td style="text-align:left;">DiT-S</td>
                          <td>215.76</td>
                          <td><strong>51.74</strong></td>
                        </tr>
                        <tr>
                          <td style="text-align:left;">DiT-XL</td>
                          <td>23.08</td>
                          <td><strong>7.13</strong></td>
                        </tr>
                      </table>
                      <figcaption style="font-size: 1em; text-align:center; margin-top:10px;">Standard DiT struggles to model RAE's latent distribution.</figcaption>
                    </figure>
                  </td>
                </tr>
              </table>
            </div>

            <div class="finding-box", style="margin-top: -20px;">
              <ul>
                <li><strong>Suboptimal design for diffusion transformers.</strong> When modeling high-dimensional RAE tokens, the optimal design choices for diffusion transformers can diverge from the standard DiT, which was originally tailored for low-dimensional VAE tokens.</li>
                <li><strong>Suboptimal noise scheduling.</strong> Prior noise scheduling and loss re-weighting tricks are derived for image-based or VAE-based input, and it remains unclear if they transfer well to high-dimension semantic tokens.</li>
                <li><strong>Diffusion generates noisy latents.</strong> VAE decoders are trained to reconstruct images from noisy latents, making them more tolerant to small noises in diffusion outputs. In contrast, RAE decoders are trained on clean latents and may struggle to generalize.</li>
              </ul>
            </div>


            <h3>Scaling DiT Width to Match Token Dimensionality</h3>

            <p>
              To better understand the training dynamics of Diffusion Transformers in RAE latent space, we construct a simplified experiment:
              we randomly select <b>a single image</b>, encode it by RAE, and test whether the diffusion model can <em>reconstruct</em> it. Starting from a DiT-S, we first vary the model width while fixing depth.
              We fix the RAE encoder to DINOv2-B with token dimension of 768.
            </p>
            <p>
              As shown in the following figure<a href="#fig-overfit_width"></a>, sample quality is poor when the model width $d < $ token dimension $n=768$,
              but improves sharply and reproduces the input almost perfectly once $d >= n$. Training losses exhibit the same trend, converging only when $d >= n$.
              One might suspect that this improvement still arises from the larger model capacity, but again as shown in the following figure<a href="#fig-overfit_width"></a>,
              even when doubling the depth from 12 to 24, the generated images remain artifact-heavy, and the training losses fail to converge to similar level of $d = 768$.
            </p>
            
            <d-figure>
              <figure class="l-body">
                  <img src="images/rae/overfit_width.png" id="fig-overfit_width" alt="Overfitting analysis for different decoder widths">
                  <figcaption><b>Overfitting to a single sample.</b> Left: increasing model width lead to lower loss and better sample quality; Right: changing model depth has marginal effect on overfitting results.</figcaption>
              </figure>
            </d-figure>

            <p>
              Together, the results indicate that for generation in RAE's latent space to succeed, <b>the diffusion model's width must match or exceed the RAE's token dimension</b>.
              This appears to contradict the common belief that data usually has low intrinsic dimension <d-cite key="pope2021intrinsic"></d-cite> and thus allowing generative models like GAN to operate effectively without scaling width to the full data dimension <d-cite key="styleganxl"></d-cite>.
              We note that this is due to the nature of diffusion models, where the injected Gaussian noise during training extends the data distribution‚Äôs support to the entire space, thereby "diffusing" the data manifold into a full-rank one, requiring model capacity that scales with the full data dimensionality.
            </p>

            <div style="max-width: 1000px; margin: 0 auto;">
              <table style="width:100%; margin-top:25px; margin-left: -20px; margin-right: -30px; margin-bottom: -25px; border-collapse:separate; border-spacing:12px; border: none;">
                <tr>
                  <!-- Left column: text -->
                  <td style="vertical-align:top; width:50%; text-align:justify; font-size: 1em;">
                    <p>
                      We further extend our investigation to a more practical setting by examining three models of varying width, \{DiT-S, DiT-B, DiT-L\}. Each model is overfit on a single image encoded by {DINOv2-S, DINOv2-B, DINOv2-L}, respectively, corresponding to different token dimensions of {384, 768, 1024}.
                    </p>
                  </td>
                  <!-- Right column: table -->
                  <td style="vertical-align:top; width:40%;">
                    <figure>
                      <table class="display-table" style="width:100%; ">
                        <tr>
                          <th>&nbsp;</th>
                          <th style="text-align:center; font-size: 0.8em;">DiT-S</th>
                          <th style="text-align:center; font-size: 0.8em;">DiT-B</th>
                          <th style="text-align:center; font-size: 0.8em;">DiT-L</th>
                        </tr>
                        <tr>
                          <td style="text-align:left; font-size: 0.75em;">DINOv2-S</td>
                          <td style="font-size: 0.75em;">3.6e<sup>-2</sup> <span style="color:green;">‚úì</span></td>
                          <td style="font-size: 0.75em;">1.0e<sup>-3</sup> <span style="color:green;">‚úì</span></td>
                          <td style="font-size: 0.75em;">9.7e<sup>-4</sup> <span style="color:green;">‚úì</span></td>
                        </tr>
                        <tr>
                          <td style="text-align:left; font-size: 0.75em;">DINOv2-B</td>
                          <td style="font-size: 0.75em;">5.2e<sup>-1</sup> <span style="color:red;">‚úó</span></td>
                          <td style="font-size: 0.75em;">2.4e<sup>-2</sup> <span style="color:green;">‚úì</span></td>
                          <td style="font-size: 0.75em;">1.3e<sup>-3</sup> <span style="color:green;">‚úì</span></td>
                        </tr>
                        <tr>
                          <td style="text-align:left; font-size: 0.75em;">DINOv2-L</td>
                          <td style="font-size: 0.75em;">6.5e<sup>-1</sup> <span style="color:red;">‚úó</span></td>
                          <td style="font-size: 0.75em;">2.7e<sup>-1</sup> <span style="color:red;">‚úó</span></td>
                          <td style="font-size: 0.75em;">2.2e<sup>-2</sup> <span style="color:green;">‚úì</span></td>
                        </tr>
                      </table>
                      <figcaption style="font-size: 1em; text-align:center; margin-top:10px;"><strong>Overfitting losses.</strong></figcaption>
                    </figure>
                  </td>
                </tr>
              </table>
            </div>


            <p>As shown above, convergence occurs only when the model width is at least as large as the token dimension (e.g., DiT-B with DINOv2-B), while the loss fails to converge otherwise (e.g., DiT-S with DINOv2-B).</p>

            <div class="finding-box", style="margin-top: 0px">
              <ul>
                <li><strong>Suboptimal design for diffusion transformers.</strong>We now fix the width of DiT to be at least as large as the RAE token dimension. For RAE with the DINOv2-B encoder, we pair it with DiT-XL in our following experiments.</li>
              </ul>
            </div>


            <h3>Dimension-Dependent Noise Schedule Shift</h3>

            <p>

              Many prior works <d-cite key="teng2023relay"></d-cite>, <d-cite key="chen2023importance"></d-cite>, <d-cite key="simplediffusion"></d-cite>, <d-cite key="SD3"></d-cite> have observed that,
              for inputs $\mathbf{z} \in \mathbb{R}^{C\times H\times W}$, increasing the spatial resolution ($H \times W$) reduces information corruption at the same noise level, impairing diffusion training.
              These findings, however, are based mainly on pixel- or VAE-encoded inputs with few channels (e.g., $C \leq 16$).
              In practice, the Gaussian noise is applied to both spatial and channel dimensions; as the number of channels increases, the effective ‚Äúresolution‚Äù per token also grows, reducing information corruption further. We therefore argue that proposed resolution-dependent strategies in these prior works should be generalized to the \textit{effective data dimension}, defined as the number of tokens times their dimensionality.
            </p>



            <div style="max-width: 1000px; margin: 0 auto;">
              <table style="width:100%; margin-top:-10px; margin-left: -20px; margin-bottom: -20px; border-collapse:separate; border-spacing:12px; border: none;">
                <tr>

                  <!-- Right column: table -->
                  <td style="vertical-align:top; width:40%;">
                    <figure>
                      <table class="display-table" style="width:60%;">
                        <tr>
                          <th>&nbsp;</th>
                          <th style="text-align:center;">gFID</th>
                        </tr>
                        <tr>
                          <td style="text-align:left;">w/o shift</td>
                          <td>23.08</td>
                        </tr>
                        <tr style="background-color:#f0f0f0; color:#666;">
                          <td style="text-align:left;">w/ shift</td>
                          <td><strong>4.81</strong></td>
                        </tr>
                      </table>
                      <figcaption style="font-size: 1em; text-align:center; margin-top:10px;"><strong>Impact of schedule shift.</strong></figcaption>
                    </figure>
                  </td>
                  <!-- Left column: text -->
                  <td style="vertical-align:top; width:60%; text-align:justify; font-size: 1em;">
                    <p>
                      Specifically, We adopt the shifting strategy of<d-cite key="SD3"></d-cite>: for a schedule $t_n \in [0,1]$ and input dimensions $n,m$, the shifted timestep is defined as $t_m = \frac{\alpha t_n}{1 + (\alpha-1) t_n}$ where $\alpha = \sqrt{m/n}$ is a dimension-dependent scaling factor.
                      We follow<d-cite key="SD3"></d-cite> in using $n=4096$ as the base dimension and set $m$ to the effective data dimension of RAE.
                    </p>
                  </td>
                  
                </tr>
              </table>
            </div>

            <p>This this yields significant performance gains, showing its importance for training diffusion models in the high-dimensional RAE latent space.</p>

            <div class="finding-box", style="margin-top: 0px">
              <ul>
                <li><strong>Suboptimal noise scheduling.</strong> We now default the noise schedule to be dependent on the effective data dimension for all our following experiments.</li>
              </ul>
            </div>

            <h3>Noise-Augmented Decoding</h3>

            <p>
              Unlike VAEs, whose latents follow a continuous distribution $\mathcal{N}(\mu, \sigma^2\mathbf{I})$<d-cite key="VAE"></d-cite>,
              the RAE decoder $D$ is trained to reconstruct images from a discrete latent distribution $p(\mathbf{z}) = \sum_i \delta(\mathbf{z} - \mathbf{z}_i)$, where ${\mathbf{z}_i}$ are encoder outputs from the training set.
              At inference, diffusion-generated latents may deviate slightly from this distribution due to imperfect training or sampling<d-cite key="abuduweili2024enhancing"></d-cite>, leading to out-of-distribution artifacts and degraded sample quality.

              To address this, following prior work in Normalizing Flows<d-cite key="dinh2016density"></d-cite>, <d-cite key="ho2019flow++"></d-cite>, <d-cite key="zhai2024normalizing"></d-cite>, we add Gaussian noise $\mathbf{n} \sim \mathcal{N}(0, \sigma^2\mathbf{I})$ during decoder training.
              Instead of decoding from the clean distribution $p(\mathbf{z})$, we train on its smoothed variant $p_{\mathbf{n}}(\mathbf{z}) = \int p(\mathbf{z} - \mathbf{n})\mathcal{N}(0, \sigma^2\mathbf{I})(\mathbf{n})\mathrm{d}\mathbf{n}$, improving generalization to the denser latent space of diffusion models.
              We further sample $\sigma$ from $|\mathcal{N}(0, \tau^2)|$ to regularize training and enhance robustness.
            </p>

            <div style="max-width: 1000px; margin: 0 auto;">
              <table style="width:100%; margin-top:-10px; margin-left: -20px; margin-bottom: -20px; margin-right: -30px; border-collapse:separate; border-spacing:12px; border: none;">
                <tr>
                  <!-- Right column: table -->
                  <td style="vertical-align:top; width:40%;">
                    <figure>
                      <table class="display-table" style="width:80%; font-size:0.9em;">
                        <tr>
                          <th>&nbsp;</th>
                          <th style="text-align:center;">gFID</th>
                          <th style="text-align:center;">rFID</th>
                        </tr>
                        <tr>
                          <td style="text-align:left;"> $\mathbf{z} \sim p(\mathbf{z})$</td>
                          <td>4.81</td>
                          <td>0.49</td>
                        </tr>
                        <tr style="background-color:#f0f0f0; color:#666;">
                          <td style="text-align:left;"> $\mathbf{z} \sim p_\mathbf{n}(\mathbf{z})$</td>
                          <td><strong>4.28</strong></td>
                          <td>0.57</td>
                        </tr>
                      </table>
                      <figcaption style="font-size: 1em; text-align:center; margin-top:10px;"><strong>Impact of $p_\mathbf{n}(\mathbf{z})$.</strong></figcaption>
                    </figure>
                  </td>
                  <!-- Left column: text -->
                   <td style="vertical-align:top; width:60%; text-align:justify; font-size: 1em;">
                    <p>
                      We analyze how $p_\mathbf{n}(\mathbf{z})$ affects reconstruction and generation. As shown on the right, it improves gFID but slightly worsens rFID.
                      This trade-off is expected: adding noise smooths the latent distribution and, therefore, helps reduce OOD issues for the decoder, but also removes fine details, lowering reconstruction quality. 
                    </p>
                  </td>
                  
                </tr>
              </table>
            </div>

            <div class="finding-box", style="margin-top: 0px">
              <ul>
                <li><strong>Diffusion generates noisy latents.</strong> We now adopt the noise-augmented decoding for all our following experiments.</li>
              </ul>
            </div>
            

            <div style="max-width: 1000px; margin: 0 auto;">
              <table style="width:100%; margin-left: -20px; margin-right: -30px; border-collapse:separate; border-spacing:12px;">
                <tr>

                  <!-- Left column: text -->
                  <td style="vertical-align:top; width:55%; text-align:justify; font-size: 1em;">
                    <p>
                      Combining all the above techniques, we train a DiT-XL model on RAE latents, which achieves a gFID of 4.28 (<a href="#fig-compare_convergence">Right</a>) after only 80 epochs and 2.39 after 720 epochs.
                      With the same model size, this not only surpasses prior diffusion baselines (e.g., SiT-XL<d-cite key="sit"></d-cite>) trained on VAE latents‚Äîachieving a 47√ó training speedup‚Äîbut also outpaces recent representation-alignment methods (e.g., REPA-XL<d-cite key="repa"></d-cite>) with a 16√ó faster convergence.
                    </p>
                  </td>

                  <!-- Right column: table -->
                  <td style="vertical-align:top; width:60%;">
                    <d-figure>
                      <figure class="l-body">
                          <img src="images/rae/convergence_compare.png" alt="Convergence comparison between RAEs and SD-VAE">
                          <figcaption>Training convergence on ImageNet 256√ó256: RAEs reach strong sample quality in one tenth the number of updates.</figcaption>
                      </figure>
                  </d-figure>
                  </td>
                  
                </tr>
              </table>
            </div>

            <p>In the following sections, we investigate ways to make RAE generation more efficient and effective, pushing it toward state-of-the-art performance.</p>
            
            
            <!-- <d-figure>
                <figure class="l-body">
                    <img src="images/rae/arch_teaser.png" alt="DiT architecture operating on RAE tokens">
                    <figcaption>DiT architecture tailored to RAE tokens with resolution-aware schedule shifting and decoder-aware sampling.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/comparison.PNG" alt="Sample comparison across generative models">
                    <figcaption>Sample quality comparison against state-of-the-art diffusion baselines on ImageNet 256√ó256.</figcaption>
                </figure> -->
            <!-- </d-figure>
        </section>
        <section id="scaling">
            <h2>Scaling and Efficiency</h2>
            <p>Because the encoder is frozen, scaling RAEs primarily increases decoder width and depth, keeping training efficient. We explore decoder scaling and demonstrate smooth improvements without overfitting.</p>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/model_width.png" alt="Decoder width scaling trends">
                    <figcaption>Decoder width scaling: wider RAEs steadily improve reconstruction fidelity while remaining compute-friendly.</figcaption>
                </figure>
            </d-figure>
            <d-figure>
                <figure class="l-body">
                    <img src="images/rae/recon_grid.png" alt="Reconstruction grid across model scales">
                    <figcaption>Qualitative reconstructions across model scales show consistent detail preservation.</figcaption>
                </figure>
            </d-figure>
        </section>
        <section id="sec-ddt" style="margin-top:24px;"> -->
<section id="sec-ddt" style="margin-top:24px;">
  <h2>Wide Diffusion Head</h2>

  <p>
    As discussed in <a href="#sec-theory">Section&nbsp;2</a>, within the standard DiT framework, handling higher-dimensional RAE latents
    requires scaling up the width of the entire backbone, which quickly becomes computationally expensive.
    To overcome this limitation, we draw inspiration from DDT <d-cite key="DDT"></d-cite> and introduce the
    DDT head</strong>‚Äîa <em>shallow yet wide</em> transformer module dedicated to denoising.
    By attaching this head to a standard DiT, we effectively increase model width without incurring quadratic growth in FLOPs.
    We refer to this augmented architecture as <strong>DiT<sup>DH</sup></strong>.
  </p>

  <figure style="float:left; width:32%; margin:0.5rem 1rem 0.5rem 0;">
    <img
      src="images/DDT_arch.png"
      alt="The Wide DDT Head architecture"
      style="width:100%; height:auto; border:0;"
      onerror="this.outerHTML='<div class=&quot;todo&quot;>TODO: images/DDT_arch.png not found ‚Äî please add the image.</div>'"
    />
    <figcaption style="font-size:0.9em;"><strong>The Wide DDT Head.</strong></figcaption>
  </figure>

  <p style="margin-top:0;">
    <strong>Wide DDT Head.</strong> Formally, a DiT<sup>DH</sup> model consists of a base DiT <span class="math">\(M\)</span>
    and an additional wide, shallow transformer head <span class="math">\(H\)</span>.
    Given a noisy input <span class="math">\(x_t\)</span>, timestep <span class="math">\(t\)</span>,
    and an optional class label <span class="math">\(y\)</span>, the combined model predicts the velocity
    <span class="math">\(v_t\)</span> as
  </p>

  <div style="margin:0.4rem 0 1rem 0;">
    \[
      z_t = M(x_t \mid t, y), \qquad
      v_t = H(x_t \mid z_t, t).
    \]
  </div>

  <!-- Row 1: first two figures -->
  <div style="display:flex; gap:12px; align-items:flex-start; clear:both; margin-top:20px; margin-bottom: 20px;">
    <figure id="fig-DiTvsDiTDH" style="flex:1; margin:0;">
      <img
        src="images/ditdh_vs_ditxl.png"
        alt="DiT<sup>DH</sup> scales better than large DiT with RAE latents"
        style="width:100%; height:auto;"
        onerror="this.outerHTML='<div class=&quot;todo&quot;>TODO: images/ditdh_vs_ditxl.png not found ‚Äî please add the image.</div>'"
      />
      <figcaption style="font-size:0.85em;">
        <strong>DiT<sup>DH</sup></strong> scales much better than large DiT.
      </figcaption>
    </figure>

    <figure style="flex:1; margin:0;">
      <img
        src="images/convergence_teaser_standalone.png"
        alt="DiT<sup>DH</sup> with RAE converges faster than VAE-based methods"
        style="width:100%; height:auto;"
        onerror="this.outerHTML='<div class=&quot;todo&quot;>TODO: images/convergence_teaser_standalone.png not found ‚Äî please add the image.</div>'"
      />
      <figcaption style="font-size:0.85em;">
        <strong>DiT<sup>DH</sup></strong> converges faster than VAE-based methods.
      </figcaption>
    </figure>
  </div>

  <!-- Paragraph after first row -->
  <p>
    <strong>DiT<sup>DH</sup> converges faster than DiT.</strong> We train a series of DiT<sup>DH</sup> models with varying backbone sizes
    (DiT<sup>DH</sup>-S, B, L, and XL) on RAE latents.
    We use a 2-layer, 2048-dim DiT<sup>DH</sup> head for all models. Performance is compared against the standard DiT-XL baseline.
    As shown in <a href="#fig-DiTvsDiTDH">Figure&nbsp;1a</a>, DiT<sup>DH</sup> is substantially more FLOP-efficient than DiT.
    For example, DiT<sup>DH</sup>-B requires only \( \sim 40\% \) of the training FLOPs yet outperforms DiT-XL by a large margin;
    when scaled to DiT<sup>DH</sup>-XL under a comparable training budget, it achieves an FID of 2.16‚Äînearly half that of DiT-XL.
  </p>
<!-- New Convergence paragraph -->
<p>
  <strong>Convergence Comparison.</strong> We compare the convergence behavior of DiT<sup>DH</sup>-XL with previous state-of-the-art diffusion
  models <d-cite key="dit"></d-cite>, <d-cite key="sit"></d-cite>, <d-cite key="repa"></d-cite>,
  <d-cite key="MDTv2"></d-cite>, and <d-cite key="lgt"></d-cite> in terms of FID without guidance.
  We show the convergence curve of DiT<sup>DH</sup>-XL with
  training epochs and GFLOPs, while baseline models are plotted at their reported final performance.
  DiT<sup>DH</sup>-XL already surpasses REPA-XL, MDTv2-XL, and SiT-XL around \(5 \times 10^{10}\) GFLOPs,
  and by \(5 \times 10^{11}\) GFLOPs it achieves the best FID overall, requiring over 40√ó less compute.

  <p>
  <strong>Scaling Behavior.</strong> We compare DiT<sup>DH</sup> with recent methods across different model scales.
  Increasing the size of DiT<sup>DH</sup> consistently
  improves FID performance. The smallest model, DiT<sup>DH</sup>-S, achieves a competitive FID of 6.07‚Äîalready outperforming the
  much larger REPA-XL. Scaling up to DiT<sup>DH</sup>-B yields a substantial improvement from 6.07 to 3.38, surpassing all prior
  works of similar or even larger scale. The performance continues to improve with DiT<sup>DH</sup>-XL, reaching a new
  state-of-the-art FID of 2.16 at 80 training epochs.
</p>
  <!-- Row 2: third figure -->
  <figure style="width:100%; margin-top:16px;">
    <img
      src="images/encoder_scaling_epoch80.png"
      alt="DiT<sup>DH</sup> with RAE reaches better FID than VAE methods at all scales"
      style="width:100%; height:auto;"
      onerror="this.outerHTML='<div class=&quot;todo&quot;>TODO: images/encoder_scaling_epoch80.png not found ‚Äî please add the image.</div>'"
    />
    <figcaption style="font-size:0.85em;">
      <strong>DiT<sup>DH</sup> scalability.</strong> With RAE latents, DiT<sup>DH</sup> scales more efficiently in both training compute and model size
      than RAE-based DiT and VAE-based methods. Bubble area indicates FLOPs.
    </figcaption>
  </figure>

  <!-- Paragraph after second row -->
  <!-- <p>
    <strong>DiT<sup>DH</sup> maintains its advantage across RAE scales.</strong> We compare DiT<sup>DH</sup>-XL and DiT-XL on three RAE encoders‚Äî
    DINOv2-S, DINOv2-B, and DINOv2-L. DiT<sup>DH</sup> consistently outperforms DiT, and the advantage grows with encoder size.
    For example, with DINOv2-L, DiT<sup>DH</sup> improves FID from 6.09 to 2.73.
    We attribute this robustness to the DiT<sup>DH</sup> head: larger encoders produce higher-dimensional latents,
    which amplify the width bottleneck of DiT.
    DiT<sup>DH</sup> satisfies the width requirement discussed in <a href="#sec-theory">Section&nbsp;2</a> while keeping features compact,
    and it filters out noisy information that becomes more prevalent in high-dimensional RAE latents.
  </p> -->

  <!-- <div style="margin-top:16px;">
    <table class="display-table" style="margin-top:12px; width:32%;">
      <tr>
        <th>Model</th>
        <th colspan="3">DINOv2 <d-cite key="DINO"></d-cite></th>
      </tr>
      <tr>
        <th></th>
        <th>S</th>
        <th>B</th>
        <th>L</th>
      </tr>
      <tr>
        <td style="text-align:left;">DiT-XL</td>
        <td>3.50</td>
        <td>4.28</td>
        <td>6.09</td>
      </tr>
      <tr style="background:#f0f0f0;">
        <td style="text-align:left;"><strong>DiT<sup>DH</sup>-XL</strong></td>
        <td>2.42</td>
        <td><strong>2.16</strong></td>
        <td>2.73</td>
      </tr>
    </table>
    <p style="font-size:0.9em; margin-top:6px;">
      <em>DiT<sup>DH</sup> outperforms DiT across RAE encoder sizes (S, B, L).</em>
    </p>
  </div> -->
</section>

<section id="sec-discussions" style="margin-top:40px;">
  <h2>Discussions</h2>

  <!-- === Subsection 1 === -->
  <h3>How can RAE extend to high-resolution synthesis efficiently?</h3>
  <p>
    A central challenge in generating high-resolution images is that resolution scales with the number of tokens:
    doubling image size in each dimension requires roughly four times as many tokens. To address this, we let the
    decoder handle resolution scaling by allowing its patch size <span class="math">\(p_d\)</span> to differ from the
    encoder patch size <span class="math">\(p_e\)</span>. When \(p_d = p_e\), the output matches the input resolution;
    setting \(p_d = 2p_e\) produces a 2√ó upsampled image, reconstructing a 512√ó512 image from the same tokens used at 256√ó256.
  </p>

  <figure style="float:right; width:45%; margin:0.5rem 0 0.5rem 1rem;">
    <table class="display-table" style="width:100%; font-size:0.9em;">
      <tr>
        <th>Method</th>
        <th>#Tokens</th>
        <th>gFID ‚Üì</th>
        <th>rFID ‚Üì</th>
      </tr>
      <tr>
        <td style="text-align:left;">Direct</td>
        <td>1024</td>
        <td>1.13</td>
        <td>0.53</td>
      </tr>
      <tr style="background:#f7f7f7;">
        <td style="text-align:left;">Upsample</td>
        <td>256</td>
        <td>1.61</td>
        <td>0.97</td>
      </tr>
    </table>
    <figcaption style="font-size:0.85em; margin-top:4px;">
      <strong>Comparison on ImageNet 512√ó512.</strong> Decoder upsampling achieves competitive FID compared to
      direct 512-resolution training. Both models are trained for 400 epochs.
    </figcaption>
  </figure>

  <p>
    Since the decoder is decoupled from both the encoder and the diffusion process, we can reuse diffusion models trained at
    256√ó256 resolution, simply swapping in an upsampling decoder to produce 512√ó512 outputs without retraining.
    As shown in <a href="#tab-decoder_upsampling">Table&nbsp;1</a>, this approach slightly increases rFID but achieves
    competitive gFID, while being 4√ó more efficient than quadrupling the number of tokens.
  </p>

  <div style="clear:both;"></div>

  <!-- === Subsection 2 === -->
  <h3>Does DiT<sup>DH</sup> work without RAE?</h3>
  <p>
    In this work, we propose and study RAE and DiT<sup>DH</sup>. In <a href="#sec-theory">Section&nbsp;2</a>, we showed that
    RAE combined with DiT already brings substantial benefits, even without the additional head.
    Here, we turn the question around: can DiT<sup>DH</sup> still provide improvements without the latent space of RAE?
  </p>

  <figure style="float:left; width:40%; margin:0.5rem 1rem 0.5rem 0;">
    <table class="display-table" style="width:100%; font-size:0.9em;">
      <tr>
        <th></th>
        <th>VAE</th>
        <th>DINOv2-B</th>
      </tr>
      <tr>
        <td style="text-align:left;">DiT-XL</td>
        <td>7.13</td>
        <td>4.28</td>
      </tr>
      <tr style="background:#f7f7f7;">
        <td style="text-align:left;">DiT<sup>DH</sup>-XL</td>
        <td>11.70</td>
        <td><strong>2.16</strong></td>
      </tr>
    </table>
    <figcaption style="font-size:0.85em; margin-top:4px;">
      <strong>Performance on VAE.</strong> DiT<sup>DH</sup> yields worse FID than DiT, despite using extra compute for
      the wide head.
    </figcaption>
  </figure>

  <p>
    To investigate, we train both DiT-XL and DiT<sup>DH</sup>-XL on SD-VAE latents with a patch size of 2, alongside DINOv2-B
    for comparison, for 80 epochs, and report unguided FID. As shown in
    <a href="#tab-vae_dit_vs_ddt">Table&nbsp;2</a>, DiT<sup>DH</sup> performs even worse than DiT on SD-VAE, despite the
    additional computation introduced by the diffusion head. This indicates that the head provides little benefit in
    low-dimensional latent spaces, and its primary strength arises in high-dimensional diffusion tasks introduced by RAE.
  </p>

  <div style="clear:both;"></div>

  <!-- === Subsection 3 === -->
  <h3>The role of structured representation in high-dimensional diffusion</h3>
  <p>
    DiT<sup>DH</sup> achieves strong performance when paired with the high-dimensional latent space of RAE.
    This raises a key question: is the structured representation of RAE essential, or would DiT<sup>DH</sup> work equally well
    on unstructured high-dimensional inputs such as <em>raw pixels</em>?
  </p>

  <figure style="float:right; width:40%; margin:0.5rem 0 0.5rem 1rem;">
    <table class="display-table" style="width:100%; font-size:0.9em;">
      <tr>
        <th></th>
        <th>Pixel</th>
        <th>DINOv2-B</th>
      </tr>
      <tr>
        <td style="text-align:left;">DiT-XL</td>
        <td>51.09</td>
        <td>4.28</td>
      </tr>
      <tr style="background:#f7f7f7;">
        <td style="text-align:left;">DiT<sup>DH</sup>-XL</td>
        <td>30.56</td>
        <td><strong>2.16</strong></td>
      </tr>
    </table>
    <figcaption style="font-size:0.85em; margin-top:4px;">
      <strong>Comparison on pixel diffusion.</strong> Pixel diffusion performs far worse than diffusion on DINOv2-B
      latents.
    </figcaption>
  </figure>

  <p>
    To evaluate this, we train DiT-XL and DiT<sup>DH</sup>-XL directly on raw pixels.
    For 256√ó256 images with a patch size of 16, the resulting DiT input token dimensionality is 16√ó16√ó3 = 768,
    matching that of the DINOv2-B latents. We report unguided FID after 80 epochs. As shown in
    <a href="#tab-pixel_dit_vs_ddt">Table&nbsp;3</a>, DiT<sup>DH</sup> outperforms DiT on pixels, but both models perform far
    worse than their counterparts trained on RAE latents. These results demonstrate that high dimensionality alone is not
    sufficient‚Äîthe structured representation provided by RAE is crucial for achieving strong performance gains.
  </p>

  <div style="clear:both;"></div>
</section>
<section id="sec-conclusion" style="margin-top:40px;">
  <h2>Conclusion</h2>

  <p>
    In this work, we challenge the belief that pretrained representation encoders are too high-dimensional and too semantic
    for reconstruction or generation. We show that a frozen representation encoder, paired with a lightweight trained decoder,
    forms an effective <strong>Representation Autoencoder (RAE)</strong>. On this latent space, we train Diffusion Transformers
    in a stable and efficient way with three added components: (1) match DiT width to the encoder token dimensionality,
    (2) apply a dimension-dependent shift to the noise schedule, and (3) add decoder noise augmentation so the decoder
    handles diffusion outputs. We also introduce <strong>DiT<sup>DH</sup></strong>, a shallow-but-wide diffusion transformer
    head that increases width without quadratic compute. Empirically, RAEs enable strong visual generation: on ImageNet, our
    RAE-based DiT<sup>DH</sup>-XL achieves an FID of <strong>1.51</strong> at 256√ó256 (no guidance) and
    <strong>1.13 / 1.13</strong> at 256√ó256 and 512√ó512 (with guidance). We believe RAE latents serve as a strong candidate
    for training diffusion transformers efficiently and robustly in future generative modeling research.
  </p>
</section>

    </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{ma2024sit,<br>
                &nbsp;&nbsp;title={SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers},<br>
                &nbsp;&nbsp;author={Nanye Ma and Mark Goldstein and Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden and Saining Xie},<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={2401.08740},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
    <script src="contents_bar.js"></script> <!-- for scroll/toc -->
    </body>
</html>
